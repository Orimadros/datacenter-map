{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from geopy.geocoders import Nominatim, ArcGIS\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data + manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the raw data file\n",
    "raw_datacenters_path = Path.cwd().parent.parent / 'data' / 'raw' / 'brasil' / 'datacenter_map_scraped_brasil.csv'\n",
    "\n",
    "# Load the raw data into a DataFrame\n",
    "raw_datacenters = pd.read_csv(raw_datacenters_path)\n",
    "\n",
    "# Drop the unnamed column\n",
    "raw_datacenters = raw_datacenters.drop(columns='Unnamed: 0')\n",
    "\n",
    "# Extract the provider name from the 'name' column using regex\n",
    "provider_regex = r'(.*):'\n",
    "raw_datacenters['provider'] = raw_datacenters['name'].str.extract(provider_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the unscrambling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_field_value(value):\n",
    "    \"\"\"\n",
    "    Parse a string in the format \"{number} {category}\" into a tuple (number, category).\n",
    "    \n",
    "    Args:\n",
    "        value (str): The string to parse, e.g., \"10 Servers\" or \"5.5 MW\".\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing (number, category). If the value is NaN or doesn't match the format,\n",
    "               returns (None, None).\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None, None\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    value = value.strip()\n",
    "    \n",
    "    # Use regex to extract the number and category\n",
    "    match = re.match(r'^\\s*([\\d.]+)\\s+(.+)$', value)\n",
    "    if match:\n",
    "        number_str, category = match.groups()\n",
    "        # Convert the number to float or int based on its format\n",
    "        try:\n",
    "            number = float(number_str) if '.' in number_str else int(number_str)\n",
    "        except ValueError:\n",
    "            number = number_str  # Fallback to string if conversion fails\n",
    "        return number, category.strip()\n",
    "    return None, None\n",
    "\n",
    "def extract_fields_from_row(row):\n",
    "    \"\"\"\n",
    "    Extract structured data from field columns in a row and return a Series of key-value pairs.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: A Series where keys are categories and values are numbers.\n",
    "    \"\"\"\n",
    "    extracted_fields = {}\n",
    "    for col in field_columns:\n",
    "        value = row[col]\n",
    "        number, category = parse_field_value(value)\n",
    "        if category is not None:\n",
    "            # Keep the first occurrence of each category\n",
    "            if category not in extracted_fields:\n",
    "                extracted_fields[category] = number\n",
    "    return pd.Series(extracted_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applying the unscrambling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns that start with 'field' (these contain structured data)\n",
    "field_columns = [col for col in raw_datacenters.columns if col.startswith('field')]\n",
    "\n",
    "# Apply the extraction function to each row\n",
    "extracted_fields = raw_datacenters.apply(extract_fields_from_row, axis=1)\n",
    "\n",
    "# Merge the extracted fields back into the original DataFrame\n",
    "clean_datacenters = pd.concat(\n",
    "    [raw_datacenters.drop(columns=field_columns), extracted_fields],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collapsing MW columns into a single one + adjusting column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert both columns to numeric, coercing any non‐numeric values to NaN.\n",
    "clean_datacenters[\"MW total power\"] = pd.to_numeric(clean_datacenters[\"MW total power\"], errors=\"coerce\")\n",
    "\n",
    "# Dropping colocation products column, which contains terrible data\n",
    "clean_datacenters.drop(columns=[\"colocation products\"], inplace=True)\n",
    "\n",
    "# Standardize column names by replacing spaces with underscores\n",
    "clean_datacenters.columns = [colname.replace(' ', '_') for colname in clean_datacenters.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a regex to extract zip codes in the format 12345-678\n",
    "clean_datacenters['zipcode'] = clean_datacenters['address'].str.extract(r'(\\d{5}-\\d{3})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tqdm for pandas operations.\n",
    "tqdm.pandas()\n",
    "\n",
    "def robust_geocode(address, retries=3):\n",
    "    \"\"\"\n",
    "    Try to geocode the address using multiple geocoders.\n",
    "    If 'Brazil' is not in the address, append it.\n",
    "    Returns (latitude, longitude, full_geocoded_address) \n",
    "    or (None, None, None) if all attempts fail.\n",
    "    \"\"\"\n",
    "    geocoders = [\n",
    "        Nominatim(user_agent=\"datacenters_geocoder\"),\n",
    "        ArcGIS()\n",
    "    ]\n",
    "    # Add \"Brazil\" if not present\n",
    "    if \"Brazil\" not in address:\n",
    "        address_mod = address + \", Brazil\"\n",
    "    else:\n",
    "        address_mod = address\n",
    "\n",
    "    for geocoder in geocoders:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                location = geocoder.geocode(address_mod, timeout=10)\n",
    "                if location:\n",
    "                    return location.latitude, location.longitude, location.address\n",
    "            except Exception as e:\n",
    "                time.sleep(1)\n",
    "    return None, None, None\n",
    "\n",
    "def extract_zipcode(text):\n",
    "    \"\"\"\n",
    "    Extract a Brazilian CEP (zip code) in the format NNNNN-NNN from the text.\n",
    "    Returns the CEP if found, else None.\n",
    "    \"\"\"\n",
    "    m = re.search(r'(\\d{5}-\\d{3})', text)\n",
    "    if m:\n",
    "        return m.group(0)\n",
    "    return None\n",
    "\n",
    "def geocode_row(row):\n",
    "    \"\"\"\n",
    "    For each DataFrame row, geocode the address.\n",
    "    Also try to extract a CEP from both the original address and the geocoded address.\n",
    "    Returns a Series with latitude, longitude, and zipcode.\n",
    "    \"\"\"\n",
    "    lat, lon, full_address = robust_geocode(row['address'])\n",
    "    # Try to extract from original address first.\n",
    "    zipcode = extract_zipcode(row['address'])\n",
    "    # If not found, try the full geocoded address.\n",
    "    if not zipcode and full_address:\n",
    "        zipcode = extract_zipcode(full_address)\n",
    "    return pd.Series([lat, lon, zipcode])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f69d1b1999d4e1488ab87fd519a431e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example: apply this to your DataFrame (assumed to be clean_datacenters with an \"address\" column)\n",
    "clean_datacenters[['latitude', 'longitude', 'zipcode', 'precision']] = clean_datacenters.progress_apply(geocode_row, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_datacenters['has_number'] = clean_datacenters['address'].str.contains(r'\\d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with only city-level address info\n",
    "\n",
    "# Define addresses to remove\n",
    "bad_addresses = [\n",
    "    \"São Paulo, State of São Paulo, Brazil\",\n",
    "    \"São Paulo, Brazil\",\n",
    "    \"Vinhedo, State of São Paulo, Brazil\",\n",
    "    \"Rio de Janeiro, RJ, Brazil\",\n",
    "    \"Fortaleza - CE, Brazil\"\n",
    "]\n",
    "\n",
    "# Drop rows matching these addresses\n",
    "clean_datacenters = clean_datacenters[~clean_datacenters['address'].isin(bad_addresses)].reset_index(drop=True)\n",
    "\n",
    "clean_datacenters = clean_datacenters[clean_datacenters.provider != 'MOD Mission Critical']\n",
    "\n",
    "# Update the row where name equals the specified string\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'TAKODA: Takoda Rio de Janeiro - RJ1', ['latitude', 'longitude']] = [-22.98414658606165, -43.43158945539098]\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'Digital Realty: São Paulo SUM01 Data Center', ['latitude', 'longitude']] = [-23.50341, -46.75334]\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'Digital Realty: São Paulo SUM02 Data Center', ['latitude', 'longitude']] = [-23.49610, -46.75504]\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'Digital Realty: São Paulo SUM03 Data Center', ['latitude', 'longitude']] = [-23.49610, -46.75504]\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'Digital Realty: São Paulo SUM04 Data Center', ['latitude', 'longitude']] = [-23.49610, -46.75504]\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'Scala Data Centers: SGRUTB04', ['latitude', 'longitude']] = [-23.496797, -46.815908]\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'Elea Data Centers: BSB1 Brasília Data Center', ['latitude', 'longitude']] = [-15.788539799910135, -47.88601920163078]\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'Digital Realty: Fortaleza FTZ01 Data Center', ['latitude', 'longitude']] = [-3.830654303491639, -38.61262835299939]\n",
    "\n",
    "\n",
    "clean_datacenters.loc[clean_datacenters['name'] == 'Latitude.sh: São Paulo I', 'MW_total_power'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd().parent.parent / 'data' / 'processed' / 'brasil' / 'clean_datacenters-com_br.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_datacenters[['name', 'provider', 'MW_total_power', 'sqft_total_space', 'latitude', 'longitude', 'zipcode']].to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datacentermap.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the raw data file\n",
    "raw_datacenters_path = Path.cwd().parent.parent / 'data' / 'processed' / 'brasil' / 'first_successes_datacentermap.csv'\n",
    "\n",
    "\n",
    "# Load the raw data into a DataFrame\n",
    "raw_datacenters = pd.read_csv(raw_datacenters_path)\n",
    "\n",
    "# Drop the unnamed column\n",
    "raw_datacenters = raw_datacenters.drop(columns='Unnamed: 0')\n",
    "\n",
    "raw_datacenters = raw_datacenters.dropna(subset=['address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_stat_string(s):\n",
    "    \"\"\"\n",
    "    Parse a statistic string and return a (key, value) tuple.\n",
    "    For example:\n",
    "      - \"6 MW\"          --> (\"MW\", \"6\")\n",
    "      - \"3,493 sq.m.\"   --> (\"sq m\", \"3,493\")\n",
    "      - \"Est.  2017\"    --> (\"Est\", \"2017\")\n",
    "    \"\"\"\n",
    "    s = s.strip()\n",
    "    # Handle stats that start with \"Est\" (like establishment year)\n",
    "    if s.startswith(\"Est\"):\n",
    "        # Remove \"Est.\" and any extra spaces\n",
    "        value = s.replace(\"Est.\", \"\").strip()\n",
    "        key = \"Est\"\n",
    "        return key, value\n",
    "    # Otherwise, try to match a number at the beginning\n",
    "    m = re.match(r'([\\d,\\.]+)\\s*(.*)', s)\n",
    "    if m:\n",
    "        value = m.group(1)\n",
    "        key = m.group(2).strip().rstrip('.')  # remove trailing dot if any\n",
    "        return key, value\n",
    "    return s, None\n",
    "\n",
    "def parse_statistics(stat_str):\n",
    "    \"\"\"\n",
    "    Convert the string representation of a list (e.g. \"['6 MW', '3,493 sq.m.']\") into\n",
    "    a dictionary mapping statistic keys to their values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stat_list = ast.literal_eval(stat_str)\n",
    "    except Exception:\n",
    "        stat_list = []\n",
    "    \n",
    "    row_stats = {}\n",
    "    if isinstance(stat_list, list):\n",
    "        for item in stat_list:\n",
    "            key, value = parse_stat_string(item)\n",
    "            if key and value is not None:\n",
    "                row_stats[key] = value\n",
    "    return pd.Series(row_stats)\n",
    "\n",
    "# Apply the parsing function to the \"statistics\" column of your DataFrame\n",
    "stats_df = raw_datacenters[\"statistics\"].apply(parse_statistics)\n",
    "\n",
    "# Concatenate the resulting statistics DataFrame with your original DataFrame\n",
    "raw_datacenters = pd.concat([raw_datacenters, stats_df], axis=1)\n",
    "\n",
    "raw_datacenters.drop(['statistics', 'address'], axis=1, inplace=True)\n",
    "raw_datacenters.rename({'transformed_address':'address'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152f28f497fc4b4d95e861223217294c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tqdm for pandas operations.\n",
    "tqdm.pandas()\n",
    "\n",
    "def robust_geocode(address, retries=3):\n",
    "    \"\"\"\n",
    "    Try to geocode the address using multiple geocoders.\n",
    "    If 'Brazil' is not in the address, append it.\n",
    "    Returns (latitude, longitude, full_geocoded_address) \n",
    "    or (None, None, None) if all attempts fail.\n",
    "    \"\"\"\n",
    "    # Check that address is a non-empty string\n",
    "    if not isinstance(address, str) or not address.strip():\n",
    "        return None, None, None\n",
    "\n",
    "    # List of geocoders to try\n",
    "    geocoders = [\n",
    "        Nominatim(user_agent=\"datacenters_geocoder\", timeout=10),\n",
    "        ArcGIS(timeout=10)\n",
    "    ]\n",
    "    \n",
    "    # Append \"Brazil\" if not already present\n",
    "    address_mod = address if \"Brazil\" in address else address + \", Brazil\"\n",
    "\n",
    "    for geocoder in geocoders:\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                location = geocoder.geocode(address_mod, timeout=10)\n",
    "                if location:\n",
    "                    return location.latitude, location.longitude, location.address\n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "    return None, None, None\n",
    "\n",
    "def geocode_row(row):\n",
    "    \"\"\"\n",
    "    For each DataFrame row, geocode the address.\n",
    "    Returns a Series with latitude and longitude.\n",
    "    \"\"\"\n",
    "    lat, lon, _ = robust_geocode(row['address'])\n",
    "    return pd.Series([lat, lon], index=['latitude', 'longitude'])\n",
    "\n",
    "# Apply the geocoding function to each row of raw_datacenters.\n",
    "raw_datacenters[['latitude', 'longitude']] = raw_datacenters.progress_apply(geocode_row, axis=1)\n",
    "\n",
    "raw_datacenters.loc[raw_datacenters.url == 'https://www.datacentermap.com/brazil/tambore/scala-sgrutb01/specs/', ['latitude', 'longitude']] = [-23.493382568654592, -46.81059902826906]\n",
    "raw_datacenters.loc[raw_datacenters.url == 'https://www.datacentermap.com/brazil/tambore/scala-sgrutb03/specs/', ['latitude', 'longitude']] = [-23.492998707280595, -46.81080363272966]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datacenters_path = Path.cwd().parent.parent / 'data' / 'processed' / 'brasil' / 'processed_datacentermap.csv'\n",
    "\n",
    "processed_datacenters = raw_datacenters.to_csv(processed_datacenters_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
